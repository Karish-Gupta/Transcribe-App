{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d15d760",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\karis\\AppData\\Roaming\\Python\\Python312\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\karis\\.cache\\huggingface\\hub\\models--openai--whisper-tiny.en. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\karis\\AppData\\Roaming\\Python\\Python312\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFWhisperForConditionalGeneration.\n",
      "\n",
      "All the weights of TFWhisperForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFWhisperForConditionalGeneration for predictions without further training.\n",
      "c:\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\karis\\.cache\\huggingface\\hub\\datasets--hf-internal-testing--librispeech_asr_dummy. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Generating validation split: 100%|██████████| 73/73 [00:00<00:00, 674.41 examples/s]\n",
      "c:\\Python312\\Lib\\site-packages\\paramiko\\transport.py:236: CryptographyDeprecationWarning: Blowfish has been deprecated\n",
      "  \"class\": algorithms.Blowfish,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[50257 50362  1770    13  2264   346   353   318   262 46329   286   262\n",
      "   3504  6097    11   290   356   389  9675   284  7062   465 21443    13\n",
      "  50256]], shape=(1, 25), dtype=int32)\n",
      "<|startoftranscript|><|notimestamps|> Mr. Quilter is the apostle of the middle classes, and we are glad to welcome his gospel.<|endoftext|>\n",
      "INFO:tensorflow:Assets written to: /saved_models/tf_whisper_saved\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /saved_models/tf_whisper_saved\\assets\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import transformers\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import WhisperProcessor, WhisperFeatureExtractor, TFWhisperForConditionalGeneration, WhisperTokenizer\n",
    "\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-tiny.en\")\n",
    "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-tiny.en\", predict_timestamps=True)\n",
    "processor = WhisperProcessor(feature_extractor, tokenizer)\n",
    "model = TFWhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny.en\")\n",
    "# Loading dataset\n",
    "ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
    "\n",
    "inputs = feature_extractor(\n",
    "    ds[0][\"audio\"][\"array\"], sampling_rate=ds[0][\"audio\"][\"sampling_rate\"], return_tensors=\"tf\"\n",
    ")\n",
    "input_features = inputs.input_features\n",
    "\n",
    "# Generating Transcription\n",
    "generated_ids = model.generate(input_features=input_features)\n",
    "print(generated_ids)\n",
    "transcription = processor.tokenizer.decode(generated_ids[0])\n",
    "print(transcription)\n",
    "\n",
    "# Save the model\n",
    "model.save('/saved_models/tf_whisper_saved')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5efbb982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /saved_models/tf_whisper_saved\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /saved_models/tf_whisper_saved\\assets\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import transformers\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import WhisperProcessor, WhisperFeatureExtractor, TFWhisperForConditionalGeneration, WhisperTokenizer\n",
    "\n",
    "class GenerateModel(tf.Module):\n",
    "  def __init__(self, model):\n",
    "    super(GenerateModel, self).__init__()\n",
    "    self.model = model\n",
    "\n",
    "  @tf.function(\n",
    "    input_signature=[\n",
    "      tf.TensorSpec((1, 80, 3000), tf.float32, name=\"input_features\"),\n",
    "    ],\n",
    "  )\n",
    "  def serving(self, input_features):\n",
    "    outputs = self.model.generate(\n",
    "      input_features,\n",
    "      # change below if you think your output will be bigger\n",
    "      # aka if you have bigger transcriptions\n",
    "      # you can make it 200 for example\n",
    "      max_new_tokens=100,\n",
    "      return_dict_in_generate=True,\n",
    "    )\n",
    "    return {\"sequences\": outputs[\"sequences\"]}\n",
    "\n",
    "saved_model_dir = '/saved_models/tf_whisper_saved'\n",
    "tflite_model_path = 'whisper_english.tflite'\n",
    "\n",
    "generate_model = GenerateModel(model=model)\n",
    "tf.saved_model.save(generate_model, saved_model_dir, signatures={\"serving_default\": generate_model.serving})\n",
    "\n",
    "# Convert the model\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
    "converter.target_spec.supported_ops = [\n",
    "  tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.\n",
    "  tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.\n",
    "]\n",
    "# Learn about post training quantization\n",
    "# https://www.tensorflow.org/lite/performance/post_training_quantization\n",
    "\n",
    "# Dynamic range quantization which reduces the size of the model to 25%\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "# Float16 quantization reduces the size to 50%\n",
    "#converter.target_spec.supported_types = [tf.float16]\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the model\n",
    "with open(tflite_model_path, 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
